{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38a3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Merged: ADANIENT.csv (284 rows)\n",
      "üîÑ Merged: ADANIPORTS.csv (284 rows)\n",
      "üîÑ Merged: APOLLOHOSP.csv (284 rows)\n",
      "üîÑ Merged: ASIANPAINT.csv (284 rows)\n",
      "üîÑ Merged: AXISBANK.csv (284 rows)\n",
      "üîÑ Merged: BAJAJ-AUTO.csv (284 rows)\n",
      "üîÑ Merged: BAJAJFINSV.csv (284 rows)\n",
      "üîÑ Merged: BAJFINANCE.csv (284 rows)\n",
      "üîÑ Merged: BEL.csv (284 rows)\n",
      "üîÑ Merged: BHARTIARTL.csv (284 rows)\n",
      "üîÑ Merged: BPCL.csv (284 rows)\n",
      "üîÑ Merged: BRITANNIA.csv (284 rows)\n",
      "üîÑ Merged: CIPLA.csv (284 rows)\n",
      "üîÑ Merged: COALINDIA.csv (284 rows)\n",
      "üîÑ Merged: DRREDDY.csv (284 rows)\n",
      "üîÑ Merged: EICHERMOT.csv (284 rows)\n",
      "üîÑ Merged: GRASIM.csv (284 rows)\n",
      "üîÑ Merged: HCLTECH.csv (284 rows)\n",
      "üîÑ Merged: HDFCBANK.csv (284 rows)\n",
      "üîÑ Merged: HDFCLIFE.csv (284 rows)\n",
      "üîÑ Merged: HEROMOTOCO.csv (284 rows)\n",
      "üîÑ Merged: HINDALCO.csv (284 rows)\n",
      "üîÑ Merged: HINDUNILVR.csv (284 rows)\n",
      "üîÑ Merged: ICICIBANK.csv (284 rows)\n",
      "üîÑ Merged: INDUSINDBK.csv (284 rows)\n",
      "üîÑ Merged: INFY.csv (284 rows)\n",
      "üîÑ Merged: ITC.csv (284 rows)\n",
      "üîÑ Merged: JSWSTEEL.csv (284 rows)\n",
      "üîÑ Merged: KOTAKBANK.csv (284 rows)\n",
      "üîÑ Merged: LT.csv (284 rows)\n",
      "üîÑ Merged: M&M.csv (284 rows)\n",
      "üîÑ Merged: MARUTI.csv (284 rows)\n",
      "üîÑ Merged: NESTLEIND.csv (284 rows)\n",
      "üîÑ Merged: NTPC.csv (284 rows)\n",
      "üîÑ Merged: ONGC.csv (284 rows)\n",
      "üîÑ Merged: POWERGRID.csv (284 rows)\n",
      "üîÑ Merged: RELIANCE.csv (284 rows)\n",
      "üîÑ Merged: SBILIFE.csv (284 rows)\n",
      "üîÑ Merged: SBIN.csv (284 rows)\n",
      "üîÑ Merged: SHRIRAMFIN.csv (284 rows)\n",
      "üîÑ Merged: SUNPHARMA.csv (284 rows)\n",
      "üîÑ Merged: TATACONSUM.csv (284 rows)\n",
      "üîÑ Merged: TATAMOTORS.csv (284 rows)\n",
      "üîÑ Merged: TATASTEEL.csv (284 rows)\n",
      "üîÑ Merged: TCS.csv (284 rows)\n",
      "üîÑ Merged: TECHM.csv (284 rows)\n",
      "üîÑ Merged: TITAN.csv (284 rows)\n",
      "üîÑ Merged: TRENT.csv (284 rows)\n",
      "üîÑ Merged: ULTRACEMCO.csv (284 rows)\n",
      "üîÑ Merged: WIPRO.csv (284 rows)\n",
      "\n",
      "‚úÖ Final Combined CSV Saved: combined_stock_data.csv with 14200 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csvs(input_dir='output_csvs', output_file='combined_stock_data.csv'):\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "                print(f\"üîÑ Merged: {filename} ({len(df)} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to read {filename}: {e}\")\n",
    "\n",
    "    # Optional: Clean up/standardize column order\n",
    "    expected_cols = ['date', 'symbol', 'open', 'close', 'high', 'low', 'volume', 'month', 'sector']\n",
    "    combined_df = combined_df[[col for col in expected_cols if col in combined_df.columns]]\n",
    "\n",
    "    # Save the final combined file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Final Combined CSV Saved: {output_file} with {len(combined_df)} rows\")\n",
    "\n",
    "def main():\n",
    "    combine_csvs()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1d2f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Original rows: 14200\n",
      "‚úÖ Cleaned data saved to cleaned_stock_data.csv\n",
      "üßπ Dropped rows due to missing or invalid data: 14200 / 0 missing cleaned\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_combined_stock_data(file_path='combined_stock_data.csv', output_file='cleaned_stock_data.csv'):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"üìä Original rows: {len(df)}\")\n",
    "\n",
    "    # 1. ‚úÖ Drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 2. üßº Handle missing values (optional: drop or fill)\n",
    "    missing_before = df.isnull().sum()\n",
    "    df.dropna(subset=['symbol', 'date', 'open', 'close', 'high', 'low', 'volume'], inplace=True)\n",
    "    missing_after = df.isnull().sum()\n",
    "\n",
    "    # 3. üìÖ Convert 'date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # 4. üîÑ Ensure numeric columns are correct data types\n",
    "    num_cols = ['open', 'close', 'high', 'low', 'volume']\n",
    "    for col in num_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 5. üßπ Remove invalid rows (e.g., negative or zero volume/prices)\n",
    "    df = df[(df['volume'] > 0) & (df['close'] > 0)]\n",
    "\n",
    "    # 6. üßæ Standardize column names (lowercase, no spaces)\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # 7. üóÇÔ∏è Optional: Sort by date and symbol\n",
    "    df.sort_values(by=['symbol', 'date'], inplace=True)\n",
    "\n",
    "    # 8. üíæ Save cleaned file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Cleaned data saved to {output_file}\")\n",
    "    print(f\"üßπ Dropped rows due to missing or invalid data: {len(df)} / {missing_before.sum()} missing cleaned\")\n",
    "\n",
    "def main():\n",
    "    clean_combined_stock_data()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3dbbb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in c:\\users\\bhuva\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (9.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c227dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Uploaded 14200 rows to MySQL database 'stock_data'.\n",
      "üîí MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "def upload_to_mysql(csv_file='cleaned_stock_data.csv'):\n",
    "    # üîê MySQL connection config\n",
    "    config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': 'root',\n",
    "        'database': 'stock_data'\n",
    "    }\n",
    "\n",
    "    # Load cleaned CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO stocks (date, symbol, open, close, high, low, volume, month, sector)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Insert row by row (batch insert also possible)\n",
    "        for _, row in df.iterrows():\n",
    "            cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "        conn.commit()\n",
    "        print(f\"‚úÖ Uploaded {len(df)} rows to MySQL database 'stock_data'.\")\n",
    "        \n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"‚ùå MySQL error: {err}\")\n",
    "    finally:\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"üîí MySQL connection closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    upload_to_mysql()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5456d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total Rows in MySQL: 14200\n",
      "üìà Unique Symbols: 50\n",
      "üóìÔ∏è Date Range: 2023-10-03 to 2024-11-22\n",
      "\n",
      "üîç Sample Data:\n",
      "      id                date      symbol     open    close     high      low  \\\n",
      "0  14200 2024-11-22 05:30:00       WIPRO   561.95   571.65   573.60   557.90   \n",
      "1    284 2024-11-22 05:30:00    ADANIENT  2101.00  2228.00  2289.70  2025.00   \n",
      "2   1420 2024-11-22 05:30:00    AXISBANK  1136.65  1142.40  1147.90  1127.55   \n",
      "3   1136 2024-11-22 05:30:00  ASIANPAINT  2430.00  2472.20  2493.95  2422.95   \n",
      "4    568 2024-11-22 05:30:00  ADANIPORTS  1072.60  1136.75  1155.90  1054.00   \n",
      "\n",
      "     volume    month             sector  \n",
      "0   7366616  2024-11                 IT  \n",
      "1  20939196  2024-11       Conglomerate  \n",
      "2  16687505  2024-11         Financials  \n",
      "3   3805854  2024-11  Consumer Durables  \n",
      "4  27086168  2024-11     Transportation  \n",
      "üîí Connection closed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhuva\\AppData\\Local\\Temp\\ipykernel_19220\\3678946997.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sample_df = pd.read_sql(\"SELECT * FROM stocks ORDER BY date DESC LIMIT 5;\", conn)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "def verify_mysql_stock_data():\n",
    "    config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': 'root',\n",
    "        'database': 'stock_data'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        conn = mysql.connector.connect(**config)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # 1. Total records\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM stocks;\")\n",
    "        total_rows = cursor.fetchone()[0]\n",
    "\n",
    "        # 2. Unique symbols\n",
    "        cursor.execute(\"SELECT COUNT(DISTINCT symbol) FROM stocks;\")\n",
    "        unique_symbols = cursor.fetchone()[0]\n",
    "\n",
    "        # 3. Date range\n",
    "        cursor.execute(\"SELECT MIN(date), MAX(date) FROM stocks;\")\n",
    "        min_date, max_date = cursor.fetchone()\n",
    "\n",
    "        # 4. Sample data\n",
    "        sample_df = pd.read_sql(\"SELECT * FROM stocks ORDER BY date DESC LIMIT 5;\", conn)\n",
    "\n",
    "        print(f\"‚úÖ Total Rows in MySQL: {total_rows}\")\n",
    "        print(f\"üìà Unique Symbols: {unique_symbols}\")\n",
    "        print(f\"üóìÔ∏è Date Range: {min_date.date()} to {max_date.date()}\")\n",
    "        print(\"\\nüîç Sample Data:\")\n",
    "        print(sample_df)\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"‚ùå MySQL error: {err}\")\n",
    "    finally:\n",
    "        if conn.is_connected():\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"üîí Connection closed.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    verify_mysql_stock_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d53237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sector_mapping.csv saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define sector mapping data\n",
    "data = {\n",
    "    'symbol': [\n",
    "        'ADANIENT', 'ADANIPORTS', 'APOLLOHOSP', 'ASIANPAINT', 'AXISBANK', 'BAJAJ-AUTO',\n",
    "        'BAJAJFINSV', 'BAJFINANCE', 'BEL', 'BHARTIARTL', 'BPCL', 'BRITANNIA', 'CIPLA',\n",
    "        'COALINDIA', 'DRREDDY', 'EICHERMOT', 'GRASIM', 'HCLTECH', 'HDFCBANK', 'HDFCLIFE',\n",
    "        'HEROMOTOCO', 'HINDALCO', 'HINDUNILVR', 'ICICIBANK', 'INDUSINDBK', 'INFY', 'ITC',\n",
    "        'JSWSTEEL', 'KOTAKBANK', 'LT', 'M&M', 'MARUTI', 'NESTLEIND', 'NTPC', 'ONGC',\n",
    "        'POWERGRID', 'RELIANCE', 'SBILIFE', 'SBIN', 'SHRIRAMFIN', 'SUNPHARMA', 'TATACONSUM',\n",
    "        'TATAMOTORS', 'TATASTEEL', 'TCS', 'TECHM', 'TRENT', 'ULTRACEMCO', 'WIPRO'\n",
    "    ],\n",
    "    'sector': [\n",
    "        'Conglomerate', 'Infrastructure', 'Pharmaceuticals', 'Consumer Goods', 'Banking', 'Automobile',\n",
    "        'Finance', 'Finance', 'Defense', 'Telecom', 'Energy', 'Consumer Goods', 'Pharmaceuticals',\n",
    "        'Energy', 'Pharmaceuticals', 'Automobile', 'Cement', 'Technology', 'Banking', 'Finance',\n",
    "        'Automobile', 'Metals', 'Consumer Goods', 'Banking', 'Banking', 'Technology', 'Consumer Goods',\n",
    "        'Metals', 'Banking', 'Infrastructure', 'Automobile', 'Automobile', 'Consumer Goods', 'Utilities',\n",
    "        'Energy', 'Utilities', 'Energy', 'Finance', 'Banking', 'Finance', 'Pharmaceuticals', 'Consumer Goods',\n",
    "        'Automobile', 'Metals', 'Technology', 'Technology', 'Retail', 'Cement', 'Technology'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "sector_df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "sector_df.to_csv('sector_mapping.csv', index=False)\n",
    "\n",
    "# Confirm saved\n",
    "print(\"‚úÖ sector_mapping.csv saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aacb0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_df.to_csv('sector_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a51595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_stock_data.csv: ['date', 'symbol', 'open', 'close', 'high', 'low', 'volume', 'month', 'sector']\n",
      "sector_mapping.csv: ['symbol', 'sector']\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaned_stock_data.csv:\", pd.read_csv(\"cleaned_stock_data.csv\").columns.tolist())\n",
    "print(\"sector_mapping.csv:\", pd.read_csv(\"sector_mapping.csv\").columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97d0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
